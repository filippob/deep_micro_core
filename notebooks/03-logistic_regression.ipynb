{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Try to apply a linear regression model to the merged otu table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "import biom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from skbio.stats.composition import multiplicative_replacement, clr\n",
    "\n",
    "from src import project_directory\n",
    "from src.database import get_session, Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_session()\n",
    "logging.basicConfig()\n",
    "logging.getLogger('sqlalchemy.engine.Engine').setLevel(logging.ERROR)\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok simply load otu table and then add tissue as metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biom_file = project_directory / \"merged_results/export/table/feature-table.biom\"\n",
    "table = biom.load_table(biom_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of samples in the OTU table\n",
    "samples = table.ids(axis='sample')\n",
    "print(samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database for samples in the samples list and collect the tissue\n",
    "queried_samples = session.query(Sample).filter(Sample.sample_id.in_(samples)).all()\n",
    "sample2tissue = {sample.sample_id: sample.dataset.tissue for sample in queried_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe with the same indices as the samples\n",
    "metadata = pd.DataFrame(index=samples)\n",
    "\n",
    "# Add the tissue as new metadata\n",
    "metadata['tissue'] = metadata.index.map(sample2tissue)\n",
    "\n",
    "# Update the OTU table with the new metadata\n",
    "table.add_metadata(metadata.to_dict(orient='index'), axis='sample')\n",
    "\n",
    "# Verify that the tissue has been added correctly\n",
    "print(table.metadata(axis='sample')[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the otu table to a pandas dataframe and then add the tissue as metadata.\n",
    "Table should be transposed to have samples as rows and otus as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the OTU table to a dataframe\n",
    "otu_df = pd.DataFrame(table.matrix_data.toarray(), index=table.ids(axis='observation'), columns=table.ids(axis='sample'))\n",
    "\n",
    "# Add the tissue metadata as a new column\n",
    "otu_df = otu_df.transpose()\n",
    "otu_df['tissue'] = otu_df.index.map(sample2tissue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_df[\"tissue\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = otu_df.drop(columns=[\"tissue\"])\n",
    "y = otu_df[\"tissue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert counts to relative abundances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_relative = X.div(X.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state:\n",
    "# https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_relative, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deal with compositionality using clr transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp = multiplicative_replacement(X_train.values)\n",
    "X_test_comp = multiplicative_replacement(X_test.values)\n",
    "\n",
    "X_train_clr = clr(X_train_comp)\n",
    "X_test_clr = clr(X_test_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver=\"liblinear\", max_iter=int(os.getenv(\"MAX_ITER\", 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(model, param_grid=param_grid, n_jobs=int(os.getenv(\"MAX_CPUS\", -1)), verbose=1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_clr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test_clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same confusion matrix as before but with class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fallback: unione ordinata di y_true e y_pred per garantire tutte le etichette\n",
    "class_names = np.unique(np.concatenate([y_test.astype(str), y_pred.astype(str)]))\n",
    "\n",
    "# Ricomponi la confusion matrix usando le etichette testuali e visualizza\n",
    "cm = confusion_matrix(y_test, y_pred, labels=class_names)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", values_format='d')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, project_directory / \"notebooks/logistic_regression_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to collect the coefficients to identify the features that are more important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_model.coef_[0]\n",
    "feature_importance = pd.DataFrame({'Feature ID': X.columns, 'Coefficient': coefficients})\n",
    "feature_importance['Importance'] = np.abs(feature_importance['Coefficient'])\n",
    "feature_importance.set_index('Feature ID', inplace=True)\n",
    "feature_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to load the taxononies from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_file = project_directory / \"merged_results/export/taxonomy/taxonomy.tsv\"\n",
    "\n",
    "with open(taxonomy_file, 'r') as handle:\n",
    "    reader = csv.DictReader(handle, delimiter='\\t')\n",
    "    taxonomies = [row for row in reader]\n",
    "\n",
    "taxonomies = {row['Feature ID']: row[\"Taxon\"] for row in taxonomies}\n",
    "taxonomies = {key: value.split(\";\")[:-1] for key, value in taxonomies.items()}\n",
    "taxonomies = pd.DataFrame.from_dict(taxonomies, orient='index', columns=[f\"Level_{i}\" for i in range(1, 9)])\n",
    "taxonomies.index.name = \"Feature ID\"\n",
    "taxonomies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = feature_importance.merge(taxonomies, left_index=True, right_index=True, how=\"inner\")\n",
    "merged_df.to_csv(project_directory / \"notebooks/feature_importance.csv\", index=False)\n",
    "merged_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-micro-core-JiEBGuw_-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
