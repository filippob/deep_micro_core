{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with stratified sampling\n",
    "\n",
    "Try to apply a linear regression model to the merged otu table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "import biom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from skbio.stats.composition import multiplicative_replacement, clr\n",
    "\n",
    "from src import project_directory\n",
    "from src.database import get_session, Sample, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_session()\n",
    "logging.basicConfig()\n",
    "logging.getLogger('sqlalchemy.engine.Engine').setLevel(logging.ERROR)\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok simply load otu table and then add tissue as metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "biom_file = project_directory / \"merged_results/export/table/feature-table.biom\"\n",
    "table = biom.load_table(biom_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of samples in the OTU table\n",
    "samples = table.ids(axis='sample')\n",
    "print(samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database for samples in the samples list and collect the tissue\n",
    "queried_samples = session.query(Sample).filter(Sample.sample_id.in_(samples)).all()\n",
    "sample2tissue = {sample.sample_id: sample.dataset.tissue for sample in queried_samples}\n",
    "sample2dataset = {sample.sample_id: sample.dataset_id for sample in queried_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe with the same indices as the samples\n",
    "metadata = pd.DataFrame(index=samples)\n",
    "\n",
    "# Add the tissue as new metadata\n",
    "metadata['tissue'] = metadata.index.map(sample2tissue)\n",
    "metadata['dataset'] = metadata.index.map(sample2dataset)\n",
    "\n",
    "# Update the OTU table with the new metadata\n",
    "table.add_metadata(metadata.to_dict(orient='index'), axis='sample')\n",
    "\n",
    "# Verify that the tissue has been added correctly\n",
    "print(table.metadata(axis='sample')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the otu table to a pandas dataframe and then add the tissue as metadata.\n",
    "Table should be transposed to have samples as rows and otus as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the OTU table to a dataframe\n",
    "otu_df = pd.DataFrame(table.matrix_data.toarray(), index=table.ids(axis='observation'), columns=table.ids(axis='sample'))\n",
    "\n",
    "# Add the tissue metadata as a new column\n",
    "otu_df = otu_df.transpose()\n",
    "otu_df['tissue'] = otu_df.index.map(sample2tissue)\n",
    "otu_df['dataset'] = otu_df.index.map(sample2dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the created dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect unique tissue-dataset pairs\n",
    "tissue_dataset_table = otu_df[['tissue', 'dataset']].drop_duplicates().sort_values(['tissue', 'dataset'])\n",
    "tissue_dataset_table.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply count how many samples per tissue we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_df[\"tissue\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = otu_df[\"dataset\"]\n",
    "X = otu_df.drop(columns=[\"tissue\", \"dataset\"])\n",
    "y = otu_df[\"tissue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the point is that we have 8 datasets for 4 tissues: if I simply divide the dataset using `train_test_split, I will have in test set samples from the same dataset of the training set: this could be a data leakage. So let's try to split the dataset in\n",
    "order to not have samples in the test set from the same dataset of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(test_size=0.5, n_splits=1, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test of groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = []\n",
    "test_groups = []\n",
    "\n",
    "for group in set(groups.iloc[train_idx]):\n",
    "    group = int(group)\n",
    "    dataset = session.query(Dataset).filter(Dataset.id == group).one()\n",
    "    train_groups.append((group, dataset.tissue, dataset.project))\n",
    "print(\"train groups:\", train_groups)\n",
    "\n",
    "for group in set(groups.iloc[test_idx]):\n",
    "    group = int(group)\n",
    "    dataset = session.query(Dataset).filter(Dataset.id == group).one()\n",
    "    test_groups.append((group, dataset.tissue, dataset.project))\n",
    "print(\"test groups:\", test_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now create train and test set relyin on indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to relative abundances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_relative = X_train.div(X_train.sum(axis=1), axis=0)\n",
    "X_test_relative = X_test.div(X_test.sum(axis=1), axis=0)\n",
    "\n",
    "# check that the sum of each sample is approximately 1\n",
    "print(f\"Sum train (should be ~1.0): {X_train_relative.sum(axis=1).head()}\")\n",
    "print(f\"Sum test (should be ~1.0): {X_test_relative.sum(axis=1).head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace zeros and apply clr transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp = multiplicative_replacement(X_train_relative.values)\n",
    "X_test_comp = multiplicative_replacement(X_test_relative.values)\n",
    "\n",
    "X_train_clr = clr(X_train_comp)\n",
    "X_test_clr = clr(X_test_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver=\"liblinear\", max_iter=int(os.getenv(\"MAX_ITER\", 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(model, param_grid=param_grid, n_jobs=int(os.getenv(\"MAX_CPUS\", -1)), verbose=1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_clr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test_clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fallback: unione ordinata di y_true e y_pred per garantire tutte le etichette\n",
    "class_names = np.unique(np.concatenate([y_test.astype(str), y_pred.astype(str)]))\n",
    "\n",
    "# Ricomponi la confusion matrix usando le etichette testuali e visualizza\n",
    "cm = confusion_matrix(y_test, y_pred, labels=class_names)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", values_format='d')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, project_directory / \"notebooks/logistic_regression_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to collect the coefficients to identify the features that are more important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_model.coef_[0]\n",
    "feature_importance = pd.DataFrame({'Feature ID': X.columns, 'Coefficient': coefficients})\n",
    "feature_importance['Importance'] = np.abs(feature_importance['Coefficient'])\n",
    "feature_importance.set_index('Feature ID', inplace=True)\n",
    "feature_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to load the taxononies from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_file = project_directory / \"merged_results/export/taxonomy/taxonomy.tsv\"\n",
    "\n",
    "with open(taxonomy_file, 'r') as handle:\n",
    "    reader = csv.DictReader(handle, delimiter='\\t')\n",
    "    taxonomies = [row for row in reader]\n",
    "\n",
    "taxonomies = {row['Feature ID']: row[\"Taxon\"] for row in taxonomies}\n",
    "taxonomies = {key: value.split(\";\")[:-1] for key, value in taxonomies.items()}\n",
    "taxonomies = pd.DataFrame.from_dict(taxonomies, orient='index', columns=[f\"Level_{i}\" for i in range(1, 9)])\n",
    "taxonomies.index.name = \"Feature ID\"\n",
    "taxonomies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = feature_importance.merge(taxonomies, left_index=True, right_index=True, how=\"inner\")\n",
    "merged_df.to_csv(project_directory / \"notebooks/feature_importance.csv\", index=False)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
